{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train and Win, Monte Carlo Reinforcement Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will train your own agent to navigate and succeed in simple and complex environments. You will explore the basics of Reinforcement Learning and Monte Carlo Method. Discover better ways to train your agent and upgrade your learning algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#What's-Reinforcement-Learning?\">What's Reinforcement Learning?</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Basic-Terminology\">Basic Terminology</a></li>\n",
    "            <li><a href=\"#Reinforcement-Learning-Process\">Reinforcement Learning Process</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Frozen-Lake-Environment\">Frozen Lake Environment</a></li>\n",
    "    <li><a href=\"#Epsilon-Greedy-Policy\">Epsilon-Greedy Policy</a></li>\n",
    "    <li><a href=\"#Monte-Carlo-Method\">Monte Carlo Method</a></li>\n",
    "    <li><a href=\"#Bigger-and-More-Complicated\">Bigger and More Complicated</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Explore-the-Environment\">Exercise 1 - Explore the Environment</a></li>\n",
    "    <li><a href=\"#Exercise-2---Episode-in-the-new-Environment\">Exercise 2 - Episode in the new Environment</a></li>\n",
    "    <li><a href=\"#Exercise-3---Train-your-agent-+-Limitations-of-Monte-Carlo\">Exercise 3 - Train your agent + Limitations of Monte Carlo</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "*   Work with an OpenAI Gym environments\n",
    "*   Explain what Reinforcement Learning is\n",
    "*   Explain what Monte Carlo Method is\n",
    "*   Create an agent that uses Monte Carlo Method to play Frozen Lake\n",
    "*   Train and Test the agents using the Frozen Lake environment\n",
    "*   Improve and update your algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`gym`](https://www.gymlibrary.dev/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01) will allow us to initialize and work with the environment.\n",
    "*   [`numpy`](https://numpy.org/doc/stable/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01) will help us to work with multidimensional array object.\n",
    "*   [`math`](https://docs.python.org/3/library/math.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01) will help us to mathematical functions.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) will help us to visualize our environment and policy.\n",
    "*   [`collections`](https://docs.python.org/3/library/collections.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01) will help us to implement specialized container data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1 math==3.5\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym==0.20.0 in /home/fachri/.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fachri/.local/lib/python3.8/site-packages (from gym==0.20.0) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/fachri/.local/lib/python3.8/site-packages (from gym==0.20.0) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym==0.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PLEASE RESTART THE KERNEL*** using the kernel tab at the top left of the jupyter lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "Now we can import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n",
    "\n",
    "Here we will just define some functions that will help us to better visualize the environment and our policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(z,epsilon=0.1):\n",
    "    \"\"\"\n",
    "    This function will select the maximum argument of its input, with the probability of epsilon. Otherwise, it will randomly select the non greedy action\n",
    "    Returns:\n",
    "    Args:\n",
    "    z:numpy array representing the action function \n",
    "    epsilon: probability of selecting the non-greedy action \n",
    "    policy:policy for FrozenLake if none a random  action will be selected \n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    argmax_=np.random.choice(np.where(z==z.max())[0])\n",
    "    \n",
    "    return argmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to draw the frozen lake\n",
    "def plot(V,policy,col_ramp=1,dpi=175,draw_vals=False): \n",
    "    plt.rcParams['figure.dpi'] = dpi\n",
    "    plt.rcParams.update({'axes.edgecolor': (0.32,0.36,0.38)})\n",
    "    plt.rcParams.update({'font.size': 4 if env.env.nrow == 8 else 7})\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(1-V.reshape(env.env.nrow,env.env.ncol)**col_ramp, cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(env.env.ncol)-.5)\n",
    "    ax.set_yticks(np.arange(env.env.nrow)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    for s in range(env.nS):\n",
    "        x = s%env.env.nrow\n",
    "        y = int(s/env.env.ncol)\n",
    "        a = policy[s]\n",
    "        gray = np.array((0.32,0.36,0.38))\n",
    "        if env.desc.tolist()[y][x] == b'G': \n",
    "            plt.text(x-0.45,y-0.3, 'goal', color=(0.75,0.22,0.17), fontname='Sans', weight='bold')\n",
    "            continue\n",
    "        if a[0] > 0.0: plt.arrow(x, y, float(a[0])*-.84, 0.0, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # left\n",
    "        if a[1] > 0.0: plt.arrow(x, y, 0.0, float(a[1])*.84,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # down\n",
    "        if a[2] > 0.0: plt.arrow(x, y, float(a[2])*.84, 0.0,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # right\n",
    "        if a[3] > 0.0: plt.arrow(x, y, 0.0, float(a[3])*-.84, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # up\n",
    "        if env.desc.tolist()[y][x] == b'F': plt.text(x-0.45,y-0.3, 'ice', color=(gray*V[s]), fontname='Sans')\n",
    "        if env.desc.tolist()[y][x] == b'S': plt.text(x-0.45,y-0.3, 'start',color=(0.21,0.51,0.48), fontname='Sans', weight='bold')\n",
    "        if draw_vals and V[s]>0:\n",
    "            vstr = '{0:.1e}'.format(V[s]) if env.env.nrow == 8 else '{0:.6f}'.format(V[s])\n",
    "            plt.text(x-0.45,y+0.45, vstr, color=(gray*V[s]), fontname='Sans')\n",
    "    plt.grid(color=(0.42,0.46,0.48), linestyle=':')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.tick_params(color=(0.42,0.46,0.48),which='both',top='off',left='off',right='off',bottom='off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_star_from_Q(Q):\n",
    "    done = False\n",
    "    pi_star = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    state = env.reset() # start in top-left, = 0\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state])\n",
    "        pi_star[state,action] = 1\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return pi_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Reinforcement Learning? \n",
    "\n",
    "In general **Reinforcement Learning**  is just another machine learning method, which is based on rewarding desired actions/output and punishing for the undesired ones. Reinforcement learning models, just like people, are choosing which action to make based on the expected return of each action. You must give your model some input which include the information about current situation and possible actions, then you must reward it based on the decision that it decides to make. Reinforcement learning models learn to perform a task through repeated trial and error interactions with an environment and does so without any human intervention.\n",
    "\n",
    "### Basic Terminology \n",
    "\n",
    "**Agent**: is your reinforcement learning model, it's a decision maker and learner. \n",
    "\n",
    "**Environment**: is a world around your agent, agent learns and acts inside of it. The environment takes the action provided by the agent and returns the next\n",
    "**state** and the **reward**.\n",
    "\n",
    "**State**: is a complete description of the state of the environment. \n",
    "\n",
    "**Action**: is a way agent interacts with the environment. **Aciton Space** is the set of all possible actions. \n",
    "\n",
    "**Reward**: is a feedback from the environment, it can be negative or positive. It impacts the agent and serves as an indication to an agent of what it should achieve. Rewards are generally unknown and agents learn how to correctly estimate the rewards.\n",
    "\n",
    "**Policy**: is a rule used by an agent to decide what actions to take, given the specific state. It works as a map from state to some action can sometimes be defined as a set of probabilities for each action in the action space. \n",
    "\n",
    "\n",
    "**Value Function**: is the function that returns the expected total reward your agent can get from following the specific policy. The agent uses this value function to make decisions and learns by updating the expected reward values of the parameters of this function. In this lab we will be using state-action value function, so our function $Q(s,a)$ will take state-action pair and will return an estimated reward for taking action $a$ from state $s$. \n",
    "\n",
    "### Reinforcement Learning Process\n",
    "1. Agent plays a number of games\n",
    "2. In every game, agent chooses an **Action** form the action space by using **Policy** and **Value Function**\n",
    "3. **Action** impacts the environment and the **Reward** and the new **State** is returned to the agent.\n",
    "4. Agent keeps track of what reward it received after choosing a certain action from a certain set. \n",
    "5. After completing the game, the agent updates the estimated reward for each state and action by using the actual rewards values received while playing the game. \n",
    "6. The whole process repeats again.\n",
    "\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/reinforcement-learning-fig1-700.jpg\" width=\"30%\" alt=\"cheques image\">\n",
    "\n",
    "\n",
    "Famous RL models that play Chess, Go or Atari Games on superhuman levels, are all based on the aforementioned principles and concepts. \n",
    "\n",
    "At first, these definitions may look confusing but don't be scared, we will review and practice them more through out the lab. \n",
    "\n",
    "<p style='color: blue'>Let's try to \"solve\" a simple environment </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake Environment\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX06PMEN/ezgif-3-c088eb1282.gif\" width=\"30%\" alt=\"iris image\">\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted.If you step into one of those holes, you'll fall into the freezing water. You have to navigate across the lake and retrieve the disc. \n",
    "\n",
    "In our case, Frozen Lake will be a a grid-like environment where each tile can be: \n",
    "\n",
    "1. **S**: starting point, safe \n",
    "2. **F**: frozen surface, safe\n",
    "3. **H**: hole, fall to your doom\n",
    "4. **G**: goal, where the disc is located\n",
    "\n",
    "Important things to know are that: \n",
    "* The game ends if you step in to a hole or get to your goal\n",
    "* You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Let's initialize our environment and explore it a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to reset the environment to start working with it. This function resets the game environment to what it was when you started the game. It will return the initial state, in our case it's the first tile ($0$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our environment looks like, in our case it should be a 4 by 4 grid: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the **action space** of the environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 4 possible actions, the action space is $\\{0,1,2,3\\} $:\n",
    "* **Left**: 0 \n",
    "* **Down**: 1\n",
    "* **Right**: 2 \n",
    "* **Up**: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a random action and see what our environment returns: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a random action from the action space\n",
    "action = env.action_space.sample()\n",
    "\n",
    "#Execute this aciton \n",
    "new_observation, reward, done, prob = env.step(action)\n",
    "new_observation, reward, done, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the first observation. Currently we are on tile $4$, the reward is $0$ since we didn't get to the goal, binary parameter represents if the game is finished, and prob shows the probability of the execution of the chosen action, we have a deterministic environment, so for example if we choose to go down we'll go down. \n",
    "\n",
    "We will introduce a bit more terminology, **Episode** is an agent-environment interactions from initial to final states, so it's one game that agent plays. In addition, our agents are operating in a discrete-time game. Each time-advancing decision is a **step** (e.x. taking some action from some state). It's easy to see that each **Episode** consists of a series of steps. \n",
    "\n",
    "Now let's observe one episode as the game progresses. First we define the actions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=[2,2,1,1,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have Right, Right,Down, Down, Down, Down and Right: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = env.reset() # start a new game\n",
    "t= 0\n",
    "for i in actions: # make 6 actions\n",
    "    state, reward, done, prob = env.step(i) # step through the action and get the outputs\n",
    "\n",
    "    # here's a look at what we get back\n",
    "    print(f\"state: {state}, reward: {reward}, done: {done}, {prob}, time step {t}, action {i}\")\n",
    "    t+=1\n",
    "    env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as mentioned, only when we reach the final goal, the G tile ($index:15$) we get the desired reward of $1$ and $done$ changes form $FALSE$ to $TRUE$ since we have finished the game.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Policy\n",
    "\n",
    "If you remember, as was mentioned before, policy is just a function that defines which action our agent should take based on the current state. In our environment, a simple deterministic policy $\\pi$ for the first 4 states $\\left[0,1,2,3\\right]$ may look like: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0XOREN/Screenshot%202022-11-10%20at%2010.42.53%20AM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "Now let's clarify a few things with the title. **Epsilon**, is just some constant, ($0 \\leq \\epsilon \\leq 1$), and it will define some probability. **Greedy**, is a concept in computer science where a greedy algorithm is the one that makes a locally optimal choice at each stage. In our case, greedy policy implies that it will choose an action with the biggest estimated return. \n",
    "\n",
    "For now assume that $Q(s,a)$ is our value function, it will return an **estimated** reward based on the given state and action. Let $A$ be the action space then our policy can be simply defined: \n",
    "\n",
    "\n",
    "$$\n",
    "\\pi(s) =\n",
    "\\begin{cases}\n",
    "a = max_{a^* \\in A}Q(s,a^*)) & \\text{with probability 1-}  \\epsilon \\\\\\\\\\\\\\\\\n",
    "\\text{some }a \\in A & \\text{with probability } \\epsilon \\\\\\\\\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let's define a new python function that will follow the epsilon probability and return an action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(action,epsilon=0.1,n_actions=4):\n",
    "    ''' \n",
    "    This function takes the best estimated action, eplsilon, and action space \n",
    "    and returns some action. \n",
    "    '''\n",
    "    # generate a random number from 0 to 1.\n",
    "    number = np.random.rand(1)\n",
    "    \n",
    "    # if number is smaller than 1-epsilon then return best estimated action\n",
    "    if number<1-epsilon:\n",
    "        return action\n",
    "    # if number is bigger or equals to 1-epsilon then return some random action from the action space\n",
    "    else:\n",
    "        action=np.random.randint(n_actions)  \n",
    "        return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask if we want to maximize our returns, why can't we always use the best action, the action with the best estimated reward, what's the point of epsilon . To answer this question we will have to learn about 2 more concepts: \n",
    " \n",
    "* **Exploration** happens when the agent takes the random action to explore more opportunities, gather more information about possible actions and the environment.\n",
    "* **Exploitation** happens when the agent makes the best decision given current information, it uses the best estimated action to maximize the reward. \n",
    "\n",
    "**Epsilon** defines the trade-off between Exploration and Exploitation.  We need it because the best long-term strategy may involve short-term sacrifices and in most of the cases, agents must explore the environment and gather enough information to make the best overall decisions. It may save our agent from doing decisions that work instead of finding the best actions. \n",
    "\n",
    "Let's test our random_action function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[0,1,2,3]\n",
    "for t in test:\n",
    "    print(\"a\",t)\n",
    "    print(\"a'\",random_action(t,epsilon=0.95,n_actions=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following part explains the learning algorithm, it's ok if you don't understand all of the math and stats details. Try focusing on understanding the general structure and principles of this algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Method \n",
    "#### Let's talk about the heart of our algorithm, the Value function that we will be using and how it estimates the reward for each action given the state. \n",
    "\n",
    "Monte Carlo Method was invented by Stanislaw Ulman in 1940s, while trying to calculate the probability of a successful Canfield solitaire (He was sick and had nothing better to do). Ulman randomly lay the cards out and simply calculated the number of successful plays. We will apply the same approach to create our value function. The basic principle of Monte Carlo method can be summarized in 4 steps: \n",
    "\n",
    "1. Define the Domain of Possible inputs \n",
    "2. Generate inputs randomly from a probability distribution over the domain\n",
    "3. Perform a deterministic computation on the inputs\n",
    "4. Average the results\n",
    "\n",
    "Before we can see it in action let's define a few things. Review that **Episode** is a set of agent-environment interactions from initial to final states which constists of steps in a discrete-time game. \n",
    "\n",
    "Monte Carlo reinforcement learning learns from **episodes of experience**, it functions by setting the value function equal to the empirical mean return.\n",
    "Let's assume that we have some initialized policy $\\pi$ that our agent follows. Then let's play a game once and gain the following episode: \n",
    "\n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "Now let's look at an total expected reward of taking an Action $A_t$ in the state $S_t$, where t is some **time step**. \n",
    "\n",
    "* At **time step** $t=0$ (the first time step), the environment (including the agent) is in some state $S_t = S_0$ (the initial state), takes an action $A_t = A_0$ (the first action in the game) and receives a reward $R_t = R_0$ and the environment (including the agent) moves to a next state $S_{0+1} = S_{1}$\n",
    "\n",
    "Let's define a function $G$, which will just give us the expected total discounted reward at each time step: \n",
    "$$G(t) = R_t +\\gamma R_{t+1}+\\gamma^2 R_{t+2} + ...+ \\gamma^{k}R_{t+k}$$\n",
    "\n",
    "Discount factor $\\gamma \\in \\left[0,1\\right]$ is an important constant. We add the initial return $R_1$ as it is, without modifying the value, then to get the total reward we are adding $R_{t+1}$ but note that the value is multiplied by $0\\leq \\gamma \\leq 1$, so $R_{t+1}$ is only partially added to $R_1$, $R_{t+2}$ is multiplied by $\\gamma^2$, $R_{t+3}$ is multiplied by $\\gamma^3$ and so on. Gamma determines how much the reinforcement learning agent cares about rewards in the distant future relative to those in the immediate future. Note that if $\\gamma=0$ then total expected return will be defined just by initial reward, so agent will only learn and care about actions that produce an immediate reward. \n",
    "\n",
    "Now we can define our action-value function $Q_{\\pi}(S, A)$ for some state $S$ and action $A$ as: \n",
    "$$Q_{\\pi}(S, A) = E_{\\pi}\\left[G(t)|S_t = S, A_t = A \\right ]$$\n",
    "\n",
    "So value function returns the expected value of a total discounted reward $G(t)$ for the time step $t$ at which $S_t = S$ and $A_t = A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate it with an example. Let's say policy resulted in a set of actions $\\left[2,2,1,1,1,2\\right]$ which brings us to the goal tile (this episode was mentioned earlier in the lab). Assume the discount factor is one. We start at the end of the episode and then work backwards. We know at the last state, the reward is one, therefore at the previous state, given the action \n",
    " \n",
    "$Q_{\\pi}(S_{4}=14, A_{4}=2)=1 $\n",
    "\n",
    "This means at time step 4, and we are at state 14. If we take action two, we expect a reward of 1. For the previous time step \n",
    "\n",
    "$Q_{\\pi}(S_{3}=10, A_{4}=1)=1 $\n",
    "\n",
    "At time step 3, and we are at state 14. If we take action 1, we expect a reward of 1 and so on,complete the rest yourself you can run the following to check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = env.reset() # start a new game\n",
    "\n",
    "for t,i in enumerate([2,2,1,1,1,2]): # make 6 actions\n",
    "\n",
    "\n",
    "    state, reward, done, prob = env.step(i) # step through the action and get the outputs\n",
    "\n",
    "    # here's a look at what we get back\n",
    "    #print(f\"state: {state}, reward: {reward}, done: {done}, {prob}, time step {t}, action {i}\")\n",
    "    display(Latex(f'$Q_{{\\pi}}(S_{t}={state},A_{t}={i})=1 $'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after completing a series of episodes in the game, how can we adjust the expected values, or a bigger question is how's the learning process itself happening in Monte Carlo Method. For it we will use the concept of **Incremental means**. \n",
    "\n",
    "**Incremental means**, is just the average of values that's computed incrementally. Let $x_1, x_2,..., x_n$ be the set of values, Let $\\mu_1, \\mu_2, ... , \\mu_{n-1}$ be a sequence of means, where $\\mu_1 = x_1$, $\\mu_2 = \\dfrac{x_1+x_2}{2}$ and $\\mu_3 = \\dfrac{x_1+x_2+x_3}{3}$ and so on. Let's see how the mean is defined incrementally:\n",
    "\n",
    "\\begin{align*} \n",
    "\\mu_n &= \\frac{1}{n}\\sum_{i = 1}^{n}x_i\\\\\\\\\\\\  \n",
    "&= \\frac{1}{n} (x_n +  \\sum_{i = 1}^{n-1}x_i)\\\\\\\\\n",
    "&= \\frac{1}{n}(x_n +  (n-1)\\mu_{n-1})\\\\\\\\\n",
    "&= \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put everything together to describe the Monte Calro Method Learning Process. Let's have an episode: \n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "For each (state, action) pair we will keep track of the number of times this (state, action) was visited, let's define function $N(s_t,a_t)$, then every time we visit (state, action) we will update the visits counter and then adjust the running mean:\n",
    "\\begin{align*}\n",
    "N(S_t, A_t)&+=1\\\\\\\\\\\\\n",
    "Q(S_t, A_t)&+= \\frac{1}{N(S_t, A_t)}(G(t) - Q(S_t, A_t))\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, our state-value for the start state $S = 0$ may look something like:\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0XOREN/Screenshot%202022-11-10%20at%2011.00.18%20AM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "So now we know how to update the action-value function and how to use it in combination with out policy to maximize the rewards, it can be summarized as: \n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/Screenshot%202022-11-08%20at%201.27.51%20PM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "Monte Carlo algorithm/method is a type of **model-free** reinforcement learning, since the agent does not use predictions of the environment response, so it's not trying to create a statistical **model** of the environment.\n",
    "\n",
    "Let's implement it and try to train our model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo( environment,N_episodes=10000,epsilon=0.1, discount_factor=1,first_visit=True):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy using the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy: a dictionary of estimated policy for Frozen Lake \n",
    "    Q: a dictionary of estimated action function\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym Frozen Lake env object \n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"  \n",
    "   \n",
    "    #dictionary of estimated action function for FrozenLake\n",
    "    Q=defaultdict(float)\n",
    "    \n",
    "    # number of visits to the action function \n",
    "    NumberVisits= defaultdict(float)\n",
    "   \n",
    "    #dictionary  for policy \n",
    "    policy=defaultdict(float)\n",
    "    \n",
    "    #number  of actions \n",
    "    number_actions=environment.action_space.n\n",
    "  \n",
    "    \n",
    "    for i in range(N_episodes):\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        state=environment.reset() \n",
    "        #reward for the first state\n",
    "        reward=0.0\n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "         #check if a policy for the state exists  \n",
    "        if isinstance(policy[state],np.int64):\n",
    "            #obtain action from policy \n",
    "            action=policy[state]\n",
    "            action=random_action(action,epsilon,n_actions=4)\n",
    "\n",
    "        else:\n",
    "            #if no policy for the state exists  select a random  action  \n",
    "            action=np.random.randint(number_actions)\n",
    "        \n",
    "        \n",
    "        #append first state, reward and action\n",
    "        episode.append({'state':state , 'reward':reward,'action':action})\n",
    "\n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "\n",
    "                #take action and find next state, reward and check if the episode is  done (True)\n",
    "                (state, reward, done, prob) = environment.step(action)\n",
    "\n",
    "                #check if a policy for the state exists  \n",
    "                if isinstance(policy[state],np.int64):\n",
    "                    #obtain action from policy\n",
    "                    action=int(policy[state])\n",
    "                    action=random_action(action,epsilon,n_actions=4)\n",
    "\n",
    "                else:\n",
    "                     #if no policy for the state exists  select a random  action  \n",
    "                    action=np.random.randint(number_actions)\n",
    "                    \n",
    "                #add state reward and action to list \n",
    "                episode.append({'state':state , 'reward':reward,'action':action})\n",
    "                \n",
    "         #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "\n",
    "        # determine the return\n",
    "        G=0\n",
    "        flag=0\n",
    "        for t,step in enumerate(episode):\n",
    "                \n",
    "                G=discount_factor*G+step['reward']\n",
    "                \n",
    "                #increment counter for action \n",
    "                NumberVisits[step['state'],step['action']]+=1\n",
    "        \n",
    "                #if the action function value  does not exist, create an array  to store them \n",
    "                if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                    Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                #calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "                Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        \n",
    "                #update the policy to select the action funciton argment with the largest value randomly select a different action \n",
    "                policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "   \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result of Monte Carlo Method, we will train it with $1000$ episodes, epsilon of $0.2$ and discount factor of $0.8$. The training process should be rather fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = monte_carlo(env,N_episodes=1000,epsilon=0.01, discount_factor=0.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually visualize the possible path that we will take using our policy and plot the grid with chosen actions for each state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.zeros_like([0]*env.observation_space.n), pi_star_from_Q(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** sometimes the policy gives an unusable path, (ex. goes left or up from the start state), try rerunning the training code to explore different paths. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the success rate when agent is using random actions from the action space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=1000\n",
    "sum_=0\n",
    "epsilon = 0.2\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    \n",
    "    while (not(done)):\n",
    "        action=np.random.randint(env.action_space.n)\n",
    "        state,reward,done,info = env.step(action)\n",
    "    sum_+=reward\n",
    "sum_/episodes*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can find the correct path in less than $1\\%$ of cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the success rate when agent is using our trained epsilon-greedy policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=1000\n",
    "sum_=0\n",
    "epsilon = 0.2\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    \n",
    "    while (not(done)):\n",
    "        action=policy[state]\n",
    "        action=random_action(action,epsilon,n_actions=4)\n",
    "\n",
    "        state,reward,done,info = env.step(action)\n",
    "    sum_+=reward\n",
    "sum_/episodes*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around $80\\%$, that's not a bad result, what about just policy with no epsilon (exploration factor): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=1000\n",
    "sum_=0\n",
    "epsilon = 0.2\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    \n",
    "    while (not(done)):\n",
    "        action=policy[state]\n",
    "        state,reward,done,info = env.step(action)\n",
    "    sum_+=reward\n",
    "sum_/episodes*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$100\\%$ of cases, so we can see that our Agent has successfully solved the environment and exploration is not really needed at the end of the training process and only decreases the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger and More Complicated\n",
    "Let's try working with a bigger Frozen lake environment. Before we were working with $4\\times 4$, lets change it to $8 \\times 8$. Note that if before we had $4\\times 4\\times 4 = 64$ possible state action pairs (counting holes and goal tiles), now we will have $8\\times 8 \\times 4 = 256$ possible state action pairs. \n",
    "\n",
    "Let's try training our model in this bigger environment. First import the environment itself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the frozen lake gym environment\n",
    "env = gym.make('FrozenLake8x8-v1', is_slippery=False) # warning: setting slippery=True results in very complex environment dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how it looks, to visually understand the complexity of this environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our agent with the same model as the last time. It will help us to understand both the efficiency and scalability of a simple Monte Carlo algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = monte_carlo(env,N_episodes=10000,epsilon=0.2, discount_factor=0.9)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's once again visualize the path that we will take using the resulted policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.zeros_like([0]*env.observation_space.n), pi_star_from_Q(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: sometimes the code above returns an empty dictionary of all 0s for Q function. Run the code a few more times to explore different results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that simple Monte Carlo Algorithm may not be efficient enough, because in only $1000$ episodes our agent is not able to reach the only tile which returns the reward a sufficient amount of times. In this environment $G$ tile is also the furthest tile from the initial $S$ tile. It clearly demonstrates the inefficiency of a simple Monte Carlo Algorithm. Think about how you can modify the algorithm, to improve it, and avoid this sampling inefficiency. \n",
    "\n",
    "In the next section we will discuss a simple trick that may help us to improve our algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring-Starts:\n",
    "Note that in big and complicated environments not every (state,action) pair may be visited during the learning process, as could be seen in the aforementioned $8\\times8$ Frozen lake example. The possible solution to this problem may be adding **exploring-starts** method. In the beginning of each episode we are always starting in the initial state $S$, but with exploring-starts we will choose our starting state randomly. **Exploring-starts** is specifying that episodes start in a stateâ€“action pair, and that every pair has a nonzero probability of being selected at the beginning of an episode.\n",
    "\n",
    "Let's check some pseudo code, to see how exploring-starts can be implemented in to our Monte Carlo algorithm. \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX06PMEN/Screenshot%202022-11-09%20at%2012.24.33%20AM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "As you can see the main part, the only thing that changes in the code itself, is that the initial state of each episode is chosen randomly, the rest of Monte Carlo Algorithm remains unchanged. There are more tricks you can apply to Monte Carlo algorithm to make more efficient, but it has some limitations as well. Finish the exercises to explore the limitation of simple MO algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "<p style='color:blue'>Here are some exercises for you to practice what you have learned.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this lab, we were working with a relatively simple deterministic environment. In the following exercises you will explore a more complicated environment. In the original Frozen Lake game, the ice is slippery, so your actions are not be deterministic. In other words, even when our agent chooses to move in one direction, the environment can execute a movement in another direction. Let's experiment with this environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Explore the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practice explore again the Action Space and the Observation Space of the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO-Explore action space.\n",
    "#TODO-Explore observation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "env.action_space\n",
    "env.observation_space\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Episode in the new Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how the agent is going to function in this environment, let's explore one episode. Note how the non-deterministic aspect of the environment affect the episode. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = #TODO- start a new game by reseting the environment \n",
    "\n",
    "for i in [...]: # make 4-6 actions\n",
    "\n",
    "\n",
    "    state, reward, done, prob =  # step through the action and get the outputs\n",
    "\n",
    "    # Check the output:\n",
    "    print(f\"state: {state}, reward: {reward}, done: {done}, {prob}\")\n",
    "\n",
    "    env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "current_observation = env.reset() # start a new game\n",
    "\n",
    "for i in [2,2,1,1,1,2]: # make 6 actions\n",
    "\n",
    "\n",
    "    state, reward, done, prob = env.step(i) # step through the action and get the outputs\n",
    "\n",
    "    # here's a look at what we get back\n",
    "    print(f\"state: {state}, reward: {reward}, done: {done}, {prob}\")\n",
    "\n",
    "    env.render() \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Train your agent + Limitations of Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the the models, experiment with different parameters like discount factor and epsilon. Also, check how Exploring starts affect the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p, Q = #TODO - define a model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any skeleton code for example 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "policy, Q = monte_carlo(env,N_episodes=1000,epsilon=0.01, discount_factor=0.8)  \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that basically, for simple environments like this, your trained \"agent\" is summarized in just a policy function $\\pi$ that is returned by our training function and can be easily exported and used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the possible agent path that our training algorithm returns to visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.zeros_like([0]*env.observation_space.n), pi_star_from_Q(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our robot does not really perform well in the non deterministic environment, usually the plot above shows how robot slips in different direction and falls in to a hole. Our agents rarely reaches the goal tile to get a reward. Let's compare the success rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement the code first to check the sucess rate for 10000 episodeswithout trained policy and then with trained policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "episodes=10000\n",
    "sum_=0\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    \n",
    "    while (not(done)):\n",
    "        action=policy[state]\n",
    "        state,reward,done,info = env.step(action)\n",
    "    sum_+=reward\n",
    "sum_/episodes*100\n",
    "\n",
    "episodes=10000\n",
    "sum_=0\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    \n",
    "    while (not(done)):\n",
    "        action=np.random.randint(env.action_space.n)\n",
    "        state,reward,done,info = env.step(action)\n",
    "    sum_+=reward\n",
    "sum_/episodes*100\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Algorithm fails in a stochastic environment, especially when the environment is very big and complex. To work with more complex, non deterministic environments we will need a more complex model/algorithm. If you are interested, you can research more into models like Deep Q-Learning, where instead of using a simple Q function, we use a Neural Network that takes a state and approximates the Q-values for each action based on that state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! - You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n",
    "\n",
    "[Artem Arutyunov](https://www.linkedin.com/in/artem-arutyunov/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX06PMEN1305-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OpenAi Documentation and Visuals: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms and pseudocode: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction (https://www.amazon.ca/Reinforcement-Learning-Introduction-Second-Paperback/dp/B0B95WFGV6/ref=asc_df_B0B95WFGV6/?tag=googleshopc0c-20&linkCode=df0&hvadid=578919340205&hvpos=&hvnetw=g&hvrand=11011609051689383259&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9000828&hvtargid=pla-1728961664549&psc=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2022-11-17|0.1|Artem Arutyunov|Updated the Lab|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
